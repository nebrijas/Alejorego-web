{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867ca34d",
   "metadata": {},
   "source": [
    "# AD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bfeb70",
   "metadata": {},
   "source": [
    "Esta es la actividad dirigida 3 que consiste en hacer un ejercicio de programación literaria aprovechando el código que hemos usado en programación con Python donde realizamos **web scraping**. \n",
    "A continuación pongo el código fuente:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27eef6",
   "metadata": {},
   "source": [
    "# Código Fuente"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4295dbbf",
   "metadata": {},
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "resultados = []\n",
    "\n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "tags = soup.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    "\n",
    "tags = soup2.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    "\n",
    "tags = soup3.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    "\n",
    "tags = soup4.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    "\n",
    "tags = soup5.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    "\n",
    "tags = soup6.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    "\n",
    "tags = soup7.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    "\n",
    "tags = soup8.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    "\n",
    "tags = soup9.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    "\n",
    "tags = soup10.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    "\n",
    "tags = soup11.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    "\n",
    "tags = soup12.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    "\n",
    "tags = soup13.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    "\n",
    "tags = soup14.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    "\n",
    "tags = soup15.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    "\n",
    "tags = soup16.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    "\n",
    "tags = soup17.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "\n",
    "os.system(\"clear\")\n",
    "\n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a05b0",
   "metadata": {},
   "source": [
    "# Programación literaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46521df9",
   "metadata": {},
   "source": [
    "### Módulos internos del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aea0f1",
   "metadata": {},
   "source": [
    "- [time](https://docs.python.org/es/3/library/time.html), este módulo proporciona varias funciones relacionadas con el tiempo.\n",
    "- [csv](https://docs.python.org/3/library/csv.html), es el formato de importación y exportación más común para hojas de cálculo y bases de datos.\n",
    "- [re](https://docs.python.org/3/library/re.html), este módulo proporciona operaciones de coincidencia de expresiones regulares similares a las que se encuentran en Perl.\n",
    "- [os](https://docs.python.org/3/library/os.html), este módulo proporciona una forma portátil de usar la funcionalidad dependiente del sistema operativo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2d7c5",
   "metadata": {},
   "source": [
    "### Librerías externas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a91c99",
   "metadata": {},
   "source": [
    "- [requests](https://pypi.org/project/requests/), es una biblioteca HTTP simple pero elegante.\n",
    "- [bs4](https://pypi.org/project/beautifulsoup4/), es una biblioteca que facilita extraer información de páginas web.\n",
    "- [pandas](https://pypi.org/project/pandas/), es un paquete de Python que proporciona estructuras de datos rápidas, flexibles y expresivas diseñadas para que trabajar con datos \"relacionales\" o \"etiquetados\" sea fácil e intuitivo.\n",
    "- [termcolor](https://pypi.org/project/termcolor/), esta librería permite imprimir el texto coloreado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e02c4",
   "metadata": {},
   "source": [
    "### Instalamos librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16749f33",
   "metadata": {},
   "source": [
    "Las librerías que vienen con Phyton no hay que instalarlas, pero las otras sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbbc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests bs4 pandas termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7bfe84",
   "metadata": {},
   "source": [
    "### Importamos librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b628b",
   "metadata": {},
   "source": [
    "Importamos las necesarias\n",
    "- La librería `request`\n",
    "- Librería `bs4`\n",
    "- La librería `pandas`\n",
    "- La librería `termcolor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a253d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050bc18",
   "metadata": {},
   "source": [
    "### Objetos/Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb55a04",
   "metadata": {},
   "source": [
    "Creamos un objeto vacío denominado `resultados` donde pondremos los resultados del scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d419a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dde92b",
   "metadata": {},
   "source": [
    "Al pasar la función type() se observará que se trata de un objeto tipo =list= o lista de Phyton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2651879a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d3d8f2",
   "metadata": {},
   "source": [
    "### Acceder a los datos de un sitio en línea"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6369c2de",
   "metadata": {},
   "source": [
    "Para la solicitud se usará la variable req=requests.get(\"https://resultados.elpais.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83215917",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://resultados.elpais.com\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "091a13f0",
   "metadata": {},
   "source": [
    "Al pasar la función type() se observa que este objeto es de tipo respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b949d35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(req)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a0dee6",
   "metadata": {},
   "source": [
    "### Beautiful soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df605058",
   "metadata": {},
   "source": [
    "Se agrega esta variable para hacer el web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d391ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812062d5",
   "metadata": {},
   "source": [
    "Al pasar la función type() podemos observar que este objeto es de tipo bs4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "082ae9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c06fb08a",
   "metadata": {},
   "source": [
    "Agregamos la variable que sirve para encontrar todas las etiquetas <h2> en una página específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d480e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.findAll(\"h2\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3488b167",
   "metadata": {},
   "source": [
    "Al pasar la función type() se observa un objeto tags del tipo resultados modo elementos bs4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a8fb5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c45ed",
   "metadata": {},
   "source": [
    "Estas son algunas de las que practiqué en esta Actividad Dirigida 3 en Jupyter y con web scraping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
