{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d44e06",
   "metadata": {},
   "source": [
    "# AD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c067c",
   "metadata": {},
   "source": [
    "Esta es la actividad dirigida 3 que consiste en hacer un ejercicio de programación literaria aprovechando el código que hemos usado en programación con Phyton donde realizamos **web scraping**.\n",
    "A continuación pongo el código fuente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92134e9e",
   "metadata": {},
   "source": [
    "## Código fuente"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60766970",
   "metadata": {},
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    " \n",
    "resultados = []\n",
    " \n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    " \n",
    "tags = soup.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    " \n",
    "tags = soup2.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    " \n",
    "tags = soup3.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    " \n",
    "tags = soup4.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    " \n",
    "tags = soup5.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    " \n",
    "tags = soup6.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    " \n",
    "tags = soup7.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    " \n",
    "tags = soup8.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    " \n",
    "tags = soup9.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    " \n",
    "tags = soup10.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    " \n",
    "tags = soup11.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    " \n",
    "tags = soup12.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    " \n",
    "tags = soup13.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    " \n",
    "tags = soup14.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    " \n",
    "tags = soup15.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    " \n",
    "tags = soup16.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    " \n",
    "tags = soup17.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    " \n",
    "os.system(\"clear\")\n",
    " \n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7870a",
   "metadata": {},
   "source": [
    "## Programación literaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe403a",
   "metadata": {},
   "source": [
    "### Módulos del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f54078",
   "metadata": {},
   "source": [
    "- [time](https://docs.python.org/3/library/time.html), este módulo proporciona varias funciones relacionadas con el tiempo.\n",
    "- [csv](https://docs.python.org/3/library/csv.html), este es el formato de importación y exportación más común para hojas de cálculo y bases de datos.\n",
    "- [re](https://docs.python.org/3/library/re.html), proporciona operaciones de coincidencia de expresiones regulares similares a las que se encuentran en Perl.\n",
    "- [os](https://docs.python.org/3/library/os.html), proporciona una forma portátil de usar la funcionalidad dependiente del sistema operativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d52c41",
   "metadata": {},
   "source": [
    "### Librerías externas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29397d8",
   "metadata": {},
   "source": [
    "- [requests](https://pypi.org/project/requests/), es una biblioteca HTTP simple pero elegante.\n",
    "- [bs4](https://pypi.org/project/beautifulsoup4/), es una biblioteca que facilita extraer información de páginas web. Se asienta sobre un analizador HTML o XML, proporcionando modismos Pythonic para iterar, buscar y modificar el árbol de análisis.\n",
    "- [pandas](https://pypi.org/project/pandas/), es un paquete de Python que proporciona estructuras de datos rápidas, flexibles y expresivas diseñadas para que trabajar con datos \"relacionales\" o \"etiquetados\" sea fácil e intuitivo.\n",
    "- [termcolor](https://pypi.org/project/termcolor/), esta librería es para hacer uso de los colores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ee250",
   "metadata": {},
   "source": [
    "### Instalamos librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e334cd53",
   "metadata": {},
   "source": [
    "Las librerías que vienen con Phyton no hay que instalarlas pero las otras sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f92a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests bs4 pandas termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7656e1b",
   "metadata": {},
   "source": [
    "### Importamos librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592c660",
   "metadata": {},
   "source": [
    "Librerías como requests se importan tal cual, otras como pandas se importan cambiándoles el nombre y en otros casos como bs4 se importan algunos componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc38ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd55e84",
   "metadata": {},
   "source": [
    "### Objetos variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e95fa",
   "metadata": {},
   "source": [
    "Vamos a crear una serie de objetos o variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3baf5",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df499887",
   "metadata": {},
   "source": [
    "Creamos un objeto vacío denominado *resultados* donde pondremos los resultados de *scraping*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c5213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222f776",
   "metadata": {},
   "source": [
    "Si paso la función type se observará que se trata de un objeto tipo lista de Phyton que ayuda a organizar información/datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4e7a109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b99f8d",
   "metadata": {},
   "source": [
    "### Solicitud para datos de un sitio"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39f3b233",
   "metadata": {},
   "source": [
    "Variable de solicitud req=requests.get(\"https://resultados.elpais.com\") para recoger la información de esa URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca468ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://resultados.elpais.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d91cd",
   "metadata": {},
   "source": [
    "Al pasar type() se observa la respuesta en el modo requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa5eb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(req)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
